# RAG-Based Question-Answering Assistant

[ğŸ‡¹ğŸ‡· TÃ¼rkÃ§e](#tÃ¼rkÃ§e) Â· [ğŸ‡¬ğŸ‡§ English](#english)

---

## English

**[â†’ Live Demo](https://YOUR_DEMO_URL)** â€” Try it here.

RAG (Retrieval-Augmented Generation) based question-answering assistant. Upload PDFs/text files or add URLs (Wikipedia, Stack Overflow), then ask questions and get answers grounded only in those sources.

| Home | Upload & chat | Answers with sources |
|------|----------------|----------------------|
| ![Home](assets/screenshot0.png) | ![Upload](assets/screenshot1.png) | ![Chat](assets/screenshot2.png) |

### Features

- **Multiple sources:** PDF, TXT, DOCX upload and Wikipedia / Stack Overflow URLs
- **RAG pipeline:** Text chunking, embeddings, semantic search with pgvector
- **Dual providers:** OpenAI (GPT) or Gemini for chat; OpenAI or Gemini for embeddings
- **Session-based:** Chat and sources kept in browser session

### Tech stack

| Layer    | Technologies |
|----------|--------------|
| Backend  | Python, FastAPI, PostgreSQL + pgvector, OpenAI API, Google Gemini |
| Frontend | React, Vite |
| Embedding| OpenAI (text-embedding-ada-002, text-embedding-3-small), Gemini (models/gemini-embedding-001) |

### How it works

1. **Input:** User adds PDF/URL.
2. **Processing:** Text is chunked, embedded via API, stored in pgvector.
3. **Query:** User asks a question; question is embedded and similarity search runs in pgvector.
4. **Answer:** Top chunks are sent to the LLM as context; the model answers only from that context.

### Project structure

Code lives in two repositories:

| Repo      | Description | URL |
|-----------|-------------|-----|
| **Backend**  | FastAPI, pgvector, embedding & QA APIs | [answer_question_bot_backend](https://github.com/AhmetCannnn/answer_question_bot_backend) |
| **Frontend** | React UI | [answer_question_bot_frontend](https://github.com/AhmetCannnn/answer_question_bot_frontend) |

### Local setup

**Backend**

```bash
git clone https://github.com/AhmetCannnn/answer_question_bot_backend.git
cd answer_question_bot_backend
pip install -r requirements.txt
# Create .env with DATABASE_URL etc.
# Run db/schema.sql and db/seed_models.sql
uvicorn app:app --reload --port 8001
```

**Frontend**

```bash
git clone https://github.com/AhmetCannnn/answer_question_bot_frontend.git
cd answer_question_bot_frontend
npm install
npm run dev
```

Frontend runs at `http://localhost:5173`, backend at `http://localhost:8001`.

---

## TÃ¼rkÃ§e

**[â†’ CanlÄ± demo](https://YOUR_DEMO_URL)** â€” Buradan deneyebilirsiniz.

RAG (Retrieval-Augmented Generation) tabanlÄ± soru-cevap asistanÄ±. PDF/metin yÃ¼kleyebilir veya URL ekleyebilirsiniz (Wikipedia, Stack Overflow); sorularÄ±nÄ±z yalnÄ±zca bu kaynaklara dayalÄ± yanÄ±tlanÄ±r.

| Ana sayfa | Belge/URL yÃ¼kleme ve sohbet | Kaynak gÃ¶sterimli cevaplar |
|-----------|-----------------------------|----------------------------|
| ![Ana sayfa](assets/screenshot0.png) | ![Belge ve URL](assets/screenshot1.png) | ![Sohbet ve kaynaklar](assets/screenshot2.png) |

### Ã–ne Ã§Ä±kan Ã¶zellikler

- **Ã‡oklu kaynak:** PDF, TXT, DOCX yÃ¼kleme ve Wikipedia / Stack Overflow URLâ€™leri
- **RAG pipeline:** Metin chunkâ€™lama, embedding, pgvector ile semantik arama
- **Ä°ki saÄŸlayÄ±cÄ±:** Sohbet iÃ§in OpenAI (GPT) veya Gemini; embedding iÃ§in OpenAI veya Gemini
- **Session tabanlÄ±:** Sohbet ve kaynaklar tarayÄ±cÄ± oturumunda saklanÄ±r

### Teknoloji yÄ±ÄŸÄ±nÄ±

| Katman   | Teknolojiler |
|----------|---------------|
| Backend  | Python, FastAPI, PostgreSQL + pgvector, OpenAI API, Google Gemini |
| Frontend | React, Vite |
| Embedding| OpenAI (text-embedding-ada-002, text-embedding-3-small), Gemini (models/gemini-embedding-001) |

### Sistem nasÄ±l Ã§alÄ±ÅŸÄ±yor?

1. **GiriÅŸ:** KullanÄ±cÄ± PDF/URL ekler.
2. **Ä°ÅŸleme:** Metin chunkâ€™lanÄ±r, embedding API ile vektÃ¶rlenir, pgvectorâ€™e yazÄ±lÄ±r.
3. **Sorgu:** KullanÄ±cÄ± soru sorar; soru embed edilir, pgvectorâ€™de benzerlik aramasÄ± yapÄ±lÄ±r.
4. **Cevap:** En alakalÄ± chunkâ€™lar LLMâ€™e context olarak gider; model yalnÄ±zca bu baÄŸlama dayalÄ± cevap Ã¼retir.

### Proje yapÄ±sÄ±

Kod iki ayrÄ± repoâ€™da:

| Repo       | AÃ§Ä±klama | URL |
|------------|----------|-----|
| **Backend**  | FastAPI, pgvector, embedding ve QA APIâ€™leri | [answer_question_bot_backend](https://github.com/AhmetCannnn/answer_question_bot_backend) |
| **Frontend** | React arayÃ¼zÃ¼ | [answer_question_bot_frontend](https://github.com/AhmetCannnn/answer_question_bot_frontend) |

### Kurulum (yerel)

**Backend**

```bash
git clone https://github.com/AhmetCannnn/answer_question_bot_backend.git
cd answer_question_bot_backend
pip install -r requirements.txt
# .env oluÅŸturup DATABASE_URL vb. doldur
# db/schema.sql ve db/seed_models.sql Ã§alÄ±ÅŸtÄ±r
uvicorn app:app --reload --port 8001
```

**Frontend**

```bash
git clone https://github.com/AhmetCannnn/answer_question_bot_frontend.git
cd answer_question_bot_frontend
npm install
npm run dev
```

Frontend `http://localhost:5173`, backend `http://localhost:8001` Ã¼zerinde Ã§alÄ±ÅŸÄ±r.
